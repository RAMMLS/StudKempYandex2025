УДК 519.612

ФУНКЦИОНАЛЬНОСТЬ И ТЕХНОЛОГИИ
АЛГЕБРАИЧЕСКИХ РЕШАТЕЛЕЙ В БИБЛИОТЕКЕ
KRYLOV1

Д.С. Бутюгин, Я.Л. Гурьева, В.П. Ильин, Д.В. Перевозкин,
А.В. Петухов, И.Н. Скопин

Описываются функциональные возможности и особенности программной реализации
библиотеки параллельных алгоритмов Krylov, ориентированной на решение больших систем
линейных алгебраических уравнений с разреженными симметричными и несимметричными
матрицами (положительно определенными и знаконеопределенными), получаемых при се-
точных аппроксимациях многомерных краевых задач для систем дифференциальных урав-
нений на неструктурированных сетках. Библиотека включает двухуровневые итерационные
методы в подпространствах Крылова, предобуславливание которых осуществляется на осно-
ве сбалансированной декомпозиции расчетной области с различными размерами пересечений
подобластей и краевых условий сопряжения на смежных границах. Программные реализации
выполнены на типовых сжатых разреженных форматах матричных данных. Приводятся ре-
зультаты численных экспериментов с демонстрацией эффективности распараллеливания для
характерных плохо обусловленных задач. Ключевые слова: предобусловленные итерационные
алгоритмы, подпространства Крылова, методы декомпозиции областей, разреженные ал-
гебраические системы, численные эксперименты.

Введение
Настоящая работа содержит описание функционального наполнения и технологических

подходов библиотеки параллельных итерационных алгоритмов Krylov (см. предваритель-
ные публикации в [1, 2]), ориентированной на решение больших систем линейных алгеб-
раических уравнений (СЛАУ) с разреженными матрицами, возникающими при конечно-
объемных или конечно-элементных (МКО или МКЭ [3, 4]) аппроксимациях многомерных
краевых задач для систем дифференциальных уравнений на неструктурированных сетках,
на многопроцессорных вычислительных системах (МВС с количеством ядер в десятки и
сотни тысяч). В данном случае имеется в виду отсутствие программных ограничений на
проведение таких масштабных вычислительных экспериментов, а также достаточно высо-
кая эффективность применяемых алгоритмов.

Основой применяемых в библиотеке Krylov вычислительных подходов являются двух-
уровневые итерационные процессы в подпространствах Крылова, предобуславливаемые с
помощью аддитивного метода Шварца и декомпозиции расчетной области с пересечени-
ями подобластей и разными типами краевых условий на смежных внутренних границах,
см. [4–7] и цитируемые там работы.

Внешний итерационный процесс осуществляется распределенным по вычислительным
узлам образом методом FGMRES [7] с динамическими (в общем случае) предобуславливате-
лями, которые включают решение вспомогательных подсистем в расширенных подобластях
с помощью или прямого решателя PARDISO из библиотеки Intel MKL [8], или авторскими
1Статья рекомендована к публикации программным комитетом Международной научной конференции «Па-
раллельные вычислительные технологии 2013»

92 Вестник ЮУрГУ. Серия «Вычислительная математика и информатика»



Д.С. Бутюгин, Я.Л. Гурьева, В.П. Ильин, Д.В. Перевозкин, А.В. Петухов...

версиями алгоритмов BiCGStab и GMRES [7, 9], предобусловленных с помощью покомпо-
нентной или эффективной «мелкоблочной» модификациями Айзенштата неполной факто-
ризации. В последнем случае имеются в виду СЛАУ, полученные, например, при сеточных
аппроксимациях систем дифференциальных уравнений, когда одному узлу сетки соответ-
ствует несколько неизвестных функций (в задачах тепломассопереноса это могут быть три
компоненты вектора скорости, давление и температура). Отличительной чертой соответ-
ствующих матриц является то, что они представимы в блочном виде с диагональными
матрицами малого порядка, который не зависит от числа узлов сетки и, следовательно, от
порядка СЛАУ.

Коммуникации между процессорами на внешних итерациях выполняются средства-
ми системы MPI, а «внутренние» вычисления при решении СЛАУ в подобластях реали-
зуются параллельными потоками над общей памятью с помощью OpenMP. Библиотека
Krylov включает также итерационные решатели из написанной на языке CUDA библиоте-
ки Cusp [10], что позволяет решать на гетерогенном кластере внутренние подсистемы на
GPU.

Особенностью программных реализаций является то, что решаемые СЛАУ представ-
ляются в стандартном сжатом строчном формате CSR [8], который является практиче-
ски безальтернативным способом построения универсальных алгебраических решателей, но
представляет значительные трудности для эффективного распараллеливания алгоритмов.
В частности, возникает нетривиальная задача формирования CSR-форматов для вспомо-
гательных расширенных СЛАУ в подобластях.

Библиотека Krylov является функциональным аналогом таких пакетов распределенных
итерационных решателей, как например PETSc [11], Hypre [12], HIPS и pARMS [13] и др.,
однако в Krylov реализован также ряд оригинальных методов, обсуждаемых в работе.

В разделе 1 мы описываем функциональное наполнение библиотеки Krylov, в разделе 2
излагаются особенности программных реализаций алгебраических решателей, последний
же раздел посвящен численным экспериментам для характерных практических задач. В
заключении подводятся итоги работы и делаются выводы о применимости изложенных
методов.

1. Функциональное наполнение библиотеки Krylov
Рассматривается решение вещественной СЛАУ

Au = f, A = {ai,j} ∈ RN,N , (1)

матрица которой, возможно, предварительно разбита на блочные строки A = {Ap ∈
RNp,N , p = 1, ..., P}, N1 + ... + Np = N, с приблизительно равным числом строк Np  1 в
каждой. Соответствующим образом также разбиваются векторы искомого решения и пра-
вой части u = {up}, f = {fp}, так что система уравнений (1) может быть записана в форме
совокупности P подсистем меньших порядков:

∑P
Ap,pup + Ap,quq = fp, p = 1, ..., P, (2)

q=1
q 6=p

где Ap,q ∈ RNp,Nq — прямоугольные матричные блоки, получаемые при разбиении каждой
блочной строки (и всей матрицы A) на блочные столбцы.
2013, т. 2, № 3 93



Функциональность и технологии алгебраических решателей в библиотеке Krylov

Будем считать, что СЛАУ (1) представляет собой систему сеточных уравнений, так
что каждая компонента векторов u, f соответствует узлу сетки, общее число которых в

⋃P
расчетной сеточной области Ωh = Ωp равно N . При этом блочное разбиение матриц и

p=1

векторов соответствует разбиению (декомпозиции) Ωh на P сеточных непересекающихся
подобластей Ωp, в каждой из которых находится Np узлов.

Описанная декомпозиция Ωh не использует узлов-разделителей, т.е. условные грани-
цы подобластей не проходят через узлы сетки. Формальности ради можно считать, что
внешность расчетной области есть неограниченная подобласть Ω0 с числом узлов N0 = 0.

Сеточное уравнение для i-го узла сетки может быть записано в виде

∑
ai,iui + ai,juj = fi, i ∈ Ωh,

(3)
j∈ωi
j 6=i

где через ωi обозначается сеточный шаблон i-го узла, т.е. совокупность номеров всех узлов,
участвующих в i-м уравнении.

Предполагается, что сеточные узлы и соответствующие переменные пронумерованы
следующим образом: сначала идут подряд все узлы 1-й подобласти Ω1 (неважно в каком
внутреннем порядке), затем — узлы второй подобласти и т.д. Отметим, что взаимноодно-
значного соответствия алгебраической и геометрической (сеточной) интерпретации струк-
туры СЛАУ может и не существовать. Типичный пример — одному узлу сетки может со-
ответствовать m > 1 переменных в алгебраической системе. Если для каждого узла такие
«кратные» переменные нумеруются подряд, то структура СЛАУ приобретает «мелкоблоч-
ный» (m N) вид (в (3) ui и fi означают не числа, а подвекторы порядкаm соответственно,
ai,j ∈ Rm,m) и на таких случаях мы остановимся в последующем особо.

Пусть матрица (1) задана в сжатом разреженном CSR-формате [8] с указанием числа
строк Np в каждой из P подсистем (2), в соответствии с разбиением матрицы A на блоч-
ные строки Ap. Естественно, предполагается, что строки исходной матрицы пронумерованы

p∑−1 ∑p
подряд от 1 до N , так, что номера строк каждого блока Ap меняются от 1+ Ni до Ni.

i=1 i=1
На основе блочного представления СЛАУ (2) стандартным образом строится аддитив-

ный метод Шварца, на алгебраическом языке представляющий блочный итерационный ме-
тод Якоби. Однако известно, что скорость сходимости итераций возрастает, если декомпо-
зицию расчетной области сделать с пересечением областей.

В силу этого мы используем определение расширенной сеточной подобласти Ω̄p ⊃ Ωp,
имеющей пересечения с соседними подобластями, величину которых мы будем определять
в терминах количества околограничных сеточных слоев, или фронтов. Обозначим через
Γ0
p ∈ Ωp множество внутренних околограничных узлов из Ωp, т.е. таких узлов Pi ∈ Ωp, у

которых хотя бы один из соседей не лежит в Ωp(Pj ∈/ Ωp, j ∈ ωi, j 6= i)). Обозначим далее
через Γ1

p множество узлов, соседних с узлами из Γ0
p, но не принадлежащих Ωp, через⋃Γ

2
p —

множество узлов, соседних с узлами из Γ1
p, но не принадлежащих объединению Γ1

⋃ ⋃ p Ωp,
через Γ3

p — множество узлов, соседних с Γ2
p, но не принадлежащих Γ2 Γ1

p p Ωp, и т.д.
Соответственно эти множества назовем первым внешним слоем (фронтом) узлов, вторым
слоем, третьим и т.д. Получаемое объединение узлов

⋃ ⋃
Ω̄p ≡ Ω Γ1 ∆

p p... Γp (4)
94 Вестник ЮУрГУ. Серия «Вычислительная математика и информатика»



Д.С. Бутюгин, Я.Л. Гурьева, В.П. Ильин, Д.В. Перевозкин, А.В. Петухов...

будем называть расширенной p-й сеточной подобластью, а целую величину ∆ определяем
как величину расширения, или пересечения (в терминах количества сеточных слоев). Слу-
чай ∆ = 0 фактически означает декомпозицию области Ωh на подобласти без пересечений
(Ω0

p = Ωp).
На формальном алгебраическом языке каждой расширенной подобласти можно сопо-

ставить подсистему уравнений

∑P
(Āp,p + θD̄p)ūp = f̄p − Āp,qūq + θD̄pūp = r̄p, (5)

q=1
q 6=p

где ūp, f̄p, r̄p ∈ RN̄p , а D̄p — диагональная матрица, определяемая соотношением

∑P
D̄pe = Āp,qe, e = (1, ..., 1)T ∈ RN̄p . (6)

q=1
q 6=p

Здесь θ ∈ [0, 1] — некоторый итерационный параметр, который при θ = 0 соответствует
итерационному краевому условию Дирихле на смежных границах подобластей, при θ = 1 —
условию Неймана, а при 0 < θ < 1 — условию 3-го рода, или Робена.

Переходя теперь к полному вектору u и вводя матрицы

Bp = Āp,p + θD̄p, (7)

из (5) получаем итерационный аддитивный процесс Шварца в виде

un = un−1 +B−1(f −Aun−1) = un−1 +B−1rn, (8)

где B — предобуславливающая матрица, определяемая следующим образом:

B− ∑P
1 = B−1

AS = B̄−1 −1
p , B̄ 1

p = W T −
p Bp Wp. (9)

p=1

Здесь Wp : RN → RN̄p есть матрица сужения полного вектора в подвектор из Ω̄p, а W T
p —

транспонированная матрица продолжения (расширения вектора из Ω̄p в Ω).
Выполнение каждой итерации в (8) требует параллельного решения внутренних СЛАУ

в подобластях Ω̄p, что может производиться или прямым методом, или итерационным алго-
ритмом крыловского типа. В последнем случае фактически реализуются переменные (дина-
мические) предобуславливатели Bn, при этом реально вместо (8) используется универсаль-
ный «гибкий» метод обобщенных минимальных невязок FGMRES [7], оптимизирующий
невязку в подпространствах Крылова.

2. Особенности параллельной реализации алгоритмов
Описанные выше параллельные алгоритмы реализованы в форме библиотеки Krylov,

которая ориентирована на решение сверхбольших СЛАУ на системах с распределенной па-
мятью, включающих в себя большое количество вычислительных узлов. В связи с этим
реализация итерационных решателей СЛАУ и предобуславливателей имеет определенные
особенности. В частности, требуется такая организация и структура методов, которая хо-
рошо отображается на архитектуры имеющихся вычислительных систем.
2013, т. 2, № 3 95



Функциональность и технологии алгебраических решателей в библиотеке Krylov

2.1. Общая организация двухуровневых итераций

Решатели в библиотеке Krylov построены на основе широко распространенного стандар-
та интерфейса обмена данными MPI (Message Passing Interface). Базой для итерационного
решения систем уравнений в библиотеке Krylov является метод FGMRES, распределенный
по различным MPI-процессам. Данный метод совместно с различными предобуславливате-
лями типа Шварца используется для решения заданной распределенной СЛАУ.

Псевдокод алгоритма представлен на рисунке.

Метод FGMRES

Анализ вычислительной схемы алгоритма показывает, что основными в методе
FGMRES являются следующие операции:
• векторно-векторные операции (вычисление скалярных произведений, норм, и т.п.);
• матрично-векторные операции (умножение матрицы на вектор); в первых двух пунк-

тах операции выполняются в полном N -мерном пространстве, где N — размерность
решаемой СЛАУ;
• применение предобуславливателя (метод Шварца);
• QR-разложение матрицы Хессенберга, формируемой в процессе построения базисных

векторов.
Все остальные операции требуют O(1) времени и не вносят существенного вклада в

производительность решателя. Более того, вклад QR-разложения матрицы Хессенберга во
время работы решателей также можно игнорировать в предположении, что порядок ис-
ходной системы много больше количества итераций. Такое разложение также не требует

96 Вестник ЮУрГУ. Серия «Вычислительная математика и информатика»



Д.С. Бутюгин, Я.Л. Гурьева, В.П. Ильин, Д.В. Перевозкин, А.В. Петухов...

большого количества памяти, поэтому с целью уменьшения коммуникационных затрат оно
выполняется в каждом MPI-процессе.

Векторно-векторные операции в итерационных решателях распараллеливаются есте-
ственным образом за счет разбиения векторов, порождаемого декомпозицией матрицы на
расширенные блочные строки. Единственная особенность реализации этих операций за-
ключается в том, что для вычисления скалярного произведения и нормы необходима будет
точка синхронизации и обмен данными, однако, к примеру, в интерфейсе MPI имеется
требуемая функция MPI_Allreduce, реализация которой оптимизируется поставщиком
библиотеки MPI.

Для проведения матрично-векторных операций решатель строит для каждой подобла-
сти списки узлов, являющихся граничными для соседних подобластей. В дальнейшем, при
умножении матрицы на вектор или при применении итерации Шварца к вектору неизвест-
ных, решатель использует полученную информацию для пересылки частей вектора между
процессами. Ключевым моментом в таком подходе является уменьшение объема пересыла-
емых данных — необходимо пересылать в соседние подобласти только граничные элементы
вектора вместо того, чтобы пересылать вектор целиком. В случае, если разбиение на под-
области было построено подходящим способом, количество граничных вершин NΓ будет
существенно меньше размера подобласти (например, для двумерных задач NΓ будет про-
порционально квадратному корню из общего числа вершин в подобласти, а для трехмерных
NΓ ∼ N2/3).

Применение предобуславливателей, основанных на методе Шварца, требует на каждой
итерации внешнего алгоритма решения подзадач в подобластях. Достоинством метода ад-
дитивного Шварца является то, что решение в подобластях может проводиться независимо
друг от друга и не требует коммуникаций. Действительно, как уже отмечалось выше, итера-
цию Шварца можно представить в виде (8), где предобуславливатель B составлен из блоков
(7). Отсюда видно, что применение предобуславливателя B−1 в подобласти с номером p не
требует знания переменных, не принадлежащих ей.

2.2. Реализация внутренних итераций в подобластях

Для решения задач Ap,pvp = bp в подобластях можно использовать какой-нибудь прямой
решатель для разреженных систем, например решатель PARDISO из библиотеки Intel©R
MKL. Однако время, требуемое PARDISO для разложения матриц, в общем случае рас-
тет практически как O(N3/P 3), поэтому такой метод подходит только для решения не
очень больших систем на большом числе процессоров. Однако действие обратных несим-
метричных матриц A−1

p,p можно вычислять приближенно при помощи предобусловленных
методов в подпространствах Крылова, таких как GMRES и BiCGStab. В качестве предо-
буславливателя предлагается метод USOR в модификации Айзенштата, в том числе его
мелкоблочный вариант. При этом пользователю предоставляется возможность выбора ре-
шателя для подобластей — либо PARDISO из Intel©R MKL, либо одного из перечисленных
выше итерационных решателей.

Рассмотрим более подробно предобуславливатель USOR в модификации Айзенштата.
Пусть A = D − L− U , а B — предобуславливающая матрица, записанная в форме

B = (G− L)G−1(G− U) = G− L− U + LG−1U, (10)

2013, т. 2, № 3 97



Функциональность и технологии алгебраических решателей в библиотеке Krylov

где G = block-diag{Gk}, Gk ∈ Rm,m, k = 1, ...,M = N/m есть блочно-диагональная
невырожденная матрица с диагональными блоками одинакового порядка m, причем по-
рядок СЛАУ кратен m, а M — это блочный порядок матрицы A. Матрица G — некоторая
пока не конкретизируемая невырожденная матрица, для которой вычислимо разложение
G = LGUG с треугольными невырожденными множителями LG и UG.

С помощью обозначений

D̄ = L−1
G DU−1

G , L̄ = L−1
G LU−1

G , Ū = L−1
G UU−1

G (11)

предобусловленная матрица системы записывается в форме

Ā = (I − L̄)−1L−1
G AU−1

G (I − Ū)−1 =
(

= (I − L̄)−1 + (I − Ū)−1 + (I − L̄)−1(D̄ − 2I)(I − Ū)−
12)

1,

где I есть единичная матрица.
Отсюда предобусловленную СЛАУ можно представить как

Āū = f̄ ≡ L−1
G (I − L̄)−1f, ū = (I − Ū)UGu. (13)

Важно отметить, что умножение Ā на некоторый вектор v можно представить в удобной
для вычисления форме

Āv = (I − L̄)−1[v + (D̄ − 2I)w] + w, w = (I − Ū)−1v, (14)

составляющей суть модификации Айзенштата.
Конкретизируем теперь выбор матрицы G. Мы используем

1
G = D, (15)

ω

где ω — релаксационный числовой параметр. Очевидно, что такая матрица G сохраняет
блочно-диагональную структуру D. В этом случае треугольное разложение G сводится к
соответствующему разложению ее диагональных блоков:

G = block-diag{Gk = LG,kUG,k}, (16)

а реализация умножения вектора на матрицу Ā упрощается за счет блочного представления
матриц

L̄ = {L̄k,l = L−1
G,kLk,lU

−1
G,l}, Ū = {Ūk,l = L−1

G,kUk,lU
−1
G,l}, (17)

k, l = 1, ...,M.

Точнее, при использовании блочного представления векторов v = {vk}, w = {wk},
k = 1, ...,M , решения участвующих в (14) треугольных систем

(I − Ū)w = v, (I − L̄)y = z ≡ v + (D̄ − 2I)w (18)

осуществляются фактически по следующим рекуррентным формулам:

∑M
wM = vM , wk = vk + Ūk,lvl, k = M − 1, ..., 1,

l=k+1
∑k− (19)

1

y1 = z1, yk = zk + L̄k,lzl, k = 2, ...,M.
l=1

98 Вестник ЮУрГУ. Серия «Вычислительная математика и информатика»



Д.С. Бутюгин, Я.Л. Гурьева, В.П. Ильин, Д.В. Перевозкин, А.В. Петухов...

Отметим, что используемые модификации неполной факторизации являются экономич-
ными, но плохо распараллеливаемыми, вследствие необходимости решения вспомогатель-
ных СЛАУ с треугольными матрицами. Однако имеются различные подходы к распаралле-
ливанию треугольных решателей, например метод планирования вычислений по уровням,
см. [7]. Кроме того, умножение на «мелкоблочные» матрицы Uk,l и Lk,l, если они вычислены
заранее до итераций, для каждого фиксированного k позволяют обеспечить существенное
ускорение средствами OpenMP.

3. Результаты численных экспериментов
Мы приведем результаты некоторых численных экспериментов по решению ряда прак-

тических задач с помощью библиотеки Krylov, выполненных на кластере ИВМиМГ СО
РАН [14]. В данных расчетах внешние итерации проводились с помощью метода FGMRES
с критерием окончания итераций по условию на евклидовую норму невязки

||rn||2 ≤ ε||f ||2, ε = 10−7. (20)

Начальные приближения для искомых векторов всегда выбирались u0 = 0.
Вспомогательные СЛАУ в подобластях решались либо с помощью прямого решателя

PARDISO, причем наиболее трудоемкий этап — LU-разложение матрицы — выполнялся
один раз до итераций, либо с помощью итерационного метода BiCGStab с предобуславли-
вателем Айзенштата. Расчеты проводились на различном количестве P вычислительных уз-
лов, с формированием такого же числа подобластей и соответствующих им MPI-процессов.
В нижеследующих таблицах N и NZ обозначают порядок решаемых СЛАУ, а также коли-
чество ненулевых элементов матрицы, которые характеризуют трудоемкость задачи.

Рассмотрим результаты экспериментов по решению трех несимметричных СЛАУ, воз-
никающих из сеточных аппроксимаций практических задач гидро-газодинамики. Характе-
ристики соответствующих плохо обусловленных матриц даны в табл. 1.

Таблица 1
Характеристики «гидро-газодинамических» матриц

Наименование N · 10−6 NZ · 10−6

Г2 1,73 12,0
Г3 0,48 13,7
Г4 9,38 50,4

В табл. 2 — 4 приведены результаты расчетов для данных СЛАУ с использованием
PARDISO в качестве решателя в подобластях. В каждой из клеток таблиц сверху вниз
представлено общее время счета t в секундах и количество внешних итераций ne.

Здесь и далее используются следующие обозначения: Nnod — число используемых вы-
числительных узлов; Nmpi — количество MPI-процессов на одном узле; P = Nnod ·Nmpi —
общее число MPI-процессов, равное количеству подобластей; ∆ — параметр пересечения в
декомпозиции областей (количество расширений сеточной подобласти); Nth — количество
формируемых вычислительных потоков в одном MPI-процессе; T1 — время (в секундах)
решения СЛАУ без распараллеливания, т.е. при Nmpi = Nth = 1.
2013, т. 2, № 3 99



Функциональность и технологии алгебраических решателей в библиотеке Krylov

Таблица 2
Решение СЛАУ Г2 с помощью PARDISO (T1 = 87, 1)

P \∆ 0 1 2 3 4 5
60,6 39,9 33,9 30,9 28,5 27,4

5 235 116 78 59 46 37
58,4 32,9 24,6 21,0 19,1 17,4

10 486 244 167 124 99 79
118,5 47 28,6 22,6 18,3 16,0

20 934 479 330 246 194 157
230,5 74,5 53,9 33,4 27,0 20,8

30 1352 690 473 355 280 228

Таблица 3
Решение СЛАУ Г3 с помощью PARDISO, Nth = 4 (T1 = 10, 1)

P \∆ 0 1 2 3 4 5
9,45 7,46 7,23 6,80 6,75 6,52

5 82 41 29 22 18 15
5,64 4,69 4,32 4,21 4,27 4,31

10 114 59 41 32 27 23
6,03 4,09 3,72 3,76 3,77 3,59

20 164 84 58 44 37 32
6,83 4,29 3,86 3,63 3,78 3,68

30 183 94 67 52 43 38

Таблица 4
Решение СЛАУ Г4 с помощью PARDISO (T1 = 399, 4)

P \∆ 0 1 2 3 4 5
280,4 199,4 153,9 146,5 140,6 143,5

5 235 119 85 67 55 47
127,9 73,6 61,3 56,3 51,9 49,6

10 311 161 116 92 76 66
147,4 71,2 51,9 47,0 41,3 40,6

20 331 175 127 101 84 72
158,3 76,9 59,2 52,7 49,5 48,4

30 355 186 135 109 95 80

Как видно из этих результатов, использование пересечений с ростом ∆ дает стабиль-
ное уменьшение числа внешних итераций, а время счета уменьшается в два и в большее
число раз (максимальный коэффициент ускорения — свыше 10). С ростом P время вычис-
лений сначала уменьшается, но после P > 20 начинает увеличиваться, так как растет число
внешних итераций. Для исправления ситуации, очевидно, надо использовать ускорение ти-
па грубосеточной коррекции [15], которое в данных экспериментах не применялось.
100 Вестник ЮУрГУ. Серия «Вычислительная математика и информатика»



Д.С. Бутюгин, Я.Л. Гурьева, В.П. Ильин, Д.В. Перевозкин, А.В. Петухов...

Второй набор СЛАУ для численных экспериментов был представлен в блочно-строчном
распределенном формате CSR, а заданное количество блочных строк указано в табл. 5.
Там же указано, что первые две СЛАУ данного набора имеют «мелкоблочную» структуру
с размером блоков, равным m = 5. В таблице также приведено количество подобластей
P , на которые были разбиты соответствующие СЛАУ. Разбиение СЛАУ осуществлено без
пересечений, так что, фактически, в данном наборе тестов ∆ = 0.

Таблица 5
Характеристики блочных распределенных СЛАУ

Наименование N · 10−6 NZ · 10−6 P m
Г5 9,5 330 48 5
Г6 3,7 126,8 36 5
Г7 6,5 44,4 20 1
Г8 0,35 1,74 20 1

В табл. 6 приведены результаты экспериментов для четырех СЛАУ из второго набора
тестируемых задач. В клетках таблицы указано общее время счета t (слева), а также число
внешних итераций ne.

Для внутренних СЛАУ при использовании итерационных решателей фактически ис-
пользовались ограничения числа итераций nimax. Для прямого решателя PARDISO в скоб-
ках указано число запускаемых им потоков при факторизации и решении СЛАУ. В качестве
предобуславливателя для внутренних итерационных решателей использовался «мелкоблоч-
ный» предобуславливатель USOR в модификации Айзенштата. Для предобуславливателя
USOR представлены результаты без использования средств распараллеливания OpenMP.

Таблица 6
Сравнение прямых и итерационных решателей для СЛАУ в подобластях

Метод Г5 Г6 Г7 Г8
PARDISO (Nth = 8) 58,9 38 18,9 21 121,6 351 3,8 189
PARDISO (Nth = 2) 149,6 38 42,4 21 178,8 351 4,7 189

BBiCGStab (nimax = 10) 49,3 39 13,6 21 498,5 368 14,6 252
BBiCGStab (nimax = 20) 70,5 38 19,7 21 843,9 360 24,9 206

Полученные результаты позволяют сделать следующие выводы:
• на рассматриваемых СЛАУ с m = 1 прямой решатель PARDISO имеет существенное

преимущество перед итерационными, причем увеличение в нем количества использу-
емых потоков Nth от 2 до 8 дает коэффициент ускорения от 1,5 до 3;
• прямой решатель PARDISO проигрывает «мелкоблочному» итерационному алгорит-

му для СЛАУ сm = 5, хотя текущая реализация даже не использует средства OpenMP
для распараллеливания на общей памяти;
• расчеты подтверждают ожидаемый факт, что при использовании итерационных внут-

ренних решателей для СЛАУ в подобластях нет смысла добиваться высокой точности
на различных внешних итерациях; более конкретно, при увеличении количества внут-

2013, т. 2, № 3 101



Функциональность и технологии алгебраических решателей в библиотеке Krylov

ренних итераций nimax с 10 до 20 число внешних итераций несколько падает, но каждая
из них делается «дороже» и общее время решения увеличивается.

Заключение
В статье рассмотрен параллельный двухуровневый итерационный решатель, входящий

в состав библиотеки Krylov, и приведены ключевые моменты его реализации: организация
двухуровневых итераций с использованием методов Шварца и FGMRES, решение СЛАУ
в подобластях итерационными и прямыми методами. Произведены эксперименты, показы-
вающие применимость изложенных подходов к решению характерных задач. В частности,
продемонстрирована возможность использования итерационных методов с невысокой точ-
ностью для решения СЛАУ в подобластях, а также показано преимущество таких методов
в случае явного учета блочной структуры матриц.

Работа поддержана грантом РФФИ N 11-01-00205, а также грантами Президиума
РАН N 15.9-4 и ОМН РАН N 1.3.3-4.

Литература
1. Krylov: библиотека алгоритмов и программ для решения СЛАУ / Д.С. Бутюгин,

В.П. Ильин, Е.А Ицкович и др. // Современные проблемы математического моделирова-
ния. Математическое моделирование, численные методы и комплексы программ. Сбор-
ник трудов Всероссийских научных молодежных школ. Ростов-на-Дону: Изд-во Южного
федерального университета, 2009. — С. 110–128.

2. Бутюгин, Д.С. Методы параллельного решения СЛАУ на системах с распределенной
памятью в библиотеке Krylov / Д.С. Бутюгин, В.П. Ильин, Д.В. Перевозкин // Вестник
ЮУрГУ. Серия «Вычислительная математика и информатика». — 2012. — №. 47(306).
— С. 5–19.

3. Ильин, В.П. Методы конечных разностей и конечных объемов для эллиптических урав-
нений / В.П. Ильин — Новосибирск: Изд-во ИВМиМГ СО РАН, 2001. — 345 с.

4. Ильин, В.П. Методы и технологии конечных элементов / В.П. Ильин — Новосибирск:
Изд-во ИВМиМГ СО РАН, 2007. — 371 с.

5. Ильин, В.П. Параллельные методы и технологии декомпозиции областей / В.П. Ильин
// Вестник ЮУрГУ. Серия «Вычислительная математика и информатика». — 2012. —
No. 46(305). — С. 31–44.

6. Ильин, В.П. Параллельные методы декомпозиции в пространствах следов / В.П. Ильин,
Д.В. Кныш // Вычислительные методы и программирование. — 2011. — Т. 12, No. 1.
— С. 100–109.

7. Saad, Y. Iterative Methods for Sparse Linear Systems, Second Edition / Y. Saad — SIAM,
2003. — 528 p.

8. Intel Math Kernel Library. Reference Manual. URL: http://software.intel.com/
sites/products/documentation/doclib/mkl_sa/11/mklman/index.htm (дата обращения:
12.02.2013).

102 Вестник ЮУрГУ. Серия «Вычислительная математика и информатика»



Д.С. Бутюгин, Я.Л. Гурьева, В.П. Ильин, Д.В. Перевозкин, А.В. Петухов...

9. Ильин, В.П. Методы бисопряженных направлений в подпространствах Крылова /
В.П. Ильин // СибЖИМ. — 2008. — Т. 11, No. 4(36). — С. 47–60.

10. Bell, N. Cusp: Generic Parallel Algorithms for Sparse Matrix and Graph Computations
/ N. Bell, M. Garland. URL: http://cusp-library.googlecode.com (дата обращения:
15.10.2012).

11. PETSc: Home Page. URL: http://www.mcs.anl.gov/petsc/ (дата обращения:
12.02.2013).

12. Hypre. URL: http://acts.nersc.gov/hypre/ (дата обращения: 12.02.2013).
13. Yousef Saad – Software. URL: http://www-users.cs.umn.edu/~saad/software/ (дата об-

ращения: 12.02.2013).
14. Кластер НКС-30Т. URL: http://www2.sscc.ru/HKC-30T/HKC-30T.htm (дата обращения:

12.02.2013).
15. Nabben, R. A comparison of abstract versions of deflation, balancing and additive coarse

grid correction preconditioners / R. Nabben, C. Vuik // Numerical Linear Algebra with
Applications. — 2008. — Vol. 15, No. 4. — P. 355–372.

Дмитрий Сергеевич Бутюгин, младший научный сотрудник, Институт вычислительной
математики и математической геофизики СО РАН, аспирант, Новосибирский государствен-
ный университет (Новосибирск, Российская Федерация), dm.butyugin@gmail.com.

Яна Леонидовна Гурьева, к.ф.-м.н., старший научный сотрудник, Институт вычисли-
тельной математики и математической геофизики СО РАН (Новосибирск, Российская Фе-
дерация), yana@lapasrv.sscc.ru.

Валерий Павлович Ильин, д.ф.-м.н., профессор, главный научный сотрудник, Институт
вычислительной математики и математической геофизики СО РАН (Новосибирск, Россий-
ская Федерация), ilin@sscc.ru.

Данил Валерьевич Перевозкин, младший научный сотрудник, Институт вычислитель-
ной математики и математической геофизики СО РАН (Новосибирск, Российская Федера-
ция), foxillys@gmail.com.

Артем Владимирович Петухов, младший научный сотрудник, Институт вычислитель-
ной математики и математической геофизики СО РАН (Новосибирск, Российская Федера-
ция), petukhov@lapasrv.sscc.ru.

Игорь Николавевич Скопин, к.ф.-м.н., научный сотрудник, Институт вычислительной
математики и математической геофизики СО РАН (Новосибирск, Российская Федерация),
iskopin@gmail.com.

2013, т. 2, № 3 103



Функциональность и технологии алгебраических решателей в библиотеке Krylov

PARALLEL ALGEBRAIC SOLVERS LIBRARY KRYLOV
D.S. Butyugin, Institute of Computational Mathematics and Mathematical
Geophysics SB RAS (Novosibirsk, Russian Federation),
Y.L. Guryeva, Institute of Computational Mathematics and Mathematical
Geophysics SB RAS (Novosibirsk, Russian Federation),
V.P. Il’in, Institute of Computational Mathematics and Mathematical Geophysics SB
RAS (Novosibirsk, Russian Federation),
D.V. Perevozkin, Institute of Computational Mathematics and Mathematical
Geophysics SB RAS (Novosibirsk, Russian Federation),
A.V. Petukhov, Institute of Computational Mathematics and Mathematical
Geophysics SB RAS (Novosibirsk, Russian Federation),
I.N. Skopin, Institute of Computational Mathematics and Mathematical Geophysics
SB RAS (Novosibirsk, Russian Federation)

Article describes functional capabilities and software implementation peculiarities of parallel
algorithms library Krylov, which is oriented on the solution of large systems of linear algebraic
equations with sparse symmetric and unsymmetric matrices (positive definite and semi-definite)
obtained from discrete approximations of multidimensional boundary value problems for partial
differential equations on unstructured meshes. The library includes two-level iterative methods
in Krylov subspaces; preconditioning of the latter is based on the balanced decomposition of
the computational domain with variable sizes of subdomain overlapping and different boundary
conditions on interfacing boundaries. Program implementations use typical compressed sparse
matrix data formats. Results of numerical experiments are presented which demonstrate the
efficiency of parallelization for typical ill-conditioned problems.

Keywords: preconditioned iterative algorithms, Krylov subspaces, domain decomposition
methods, sparse algebraic systems, numerical experiments.

References
1. Butyugin D.S., Il’in V.P., Itskovich Y.A., et al. Krylov: biblioteka algoritmov i programm

dlya resheniya SLAU [Krylov: library of algorithms and programs for SLAEs solution].
Sovremennye problemy matematicheskogo modelirovaniya. Matematicheskoe modelirovaniye,
chislennye metody i compleksy programm. Sbornik trudov Vserossiyskikh nauchnykh
molodezhnykh shkol [Modern problems of mathematical simulation. Mathematical simulation,
numerical methods and program complexes. All-Russian young scientists schools proceedings].
Rostov-na-Donu, Publishing of South Federal University, 2009. P. 110–128.

2. Butyugin D.S. Metody parallelnogo resheniya SLAU na systemakh s raspredelennoy pamatyu
v biblioteke Krylov [Methods of parallel SLAEs solution on the systems with distributed
memory in Krylov library]. Vestnik YUURGU. Seriya “Vychislitelnaya matematika i
informatika” [Bulletin of South Ural State University. Seriers: Computational Mathematics
and Software Engineering], 2012. No. 47(306). P. 5–19.

3. Il’in V.P. Metody konechnykh raznostey i konechnykh objemov dlya ellipticheskikh uravnenij
[Methods of finite differences and finite volumes for elliptic equations] [Methods and
technologies of finite elements]. Novosibirsk, ICM&MG SBRAS Publishing, 2001.

4. Il’in V.P. Metody i tekhnologii konechnykh elementov [Methods and technologies of finite
elements]. Novosibirsk, ICM&MG SBRAS Publishing, 2007.

104 Вестник ЮУрГУ. Серия «Вычислительная математика и информатика»



Д.С. Бутюгин, Я.Л. Гурьева, В.П. Ильин, Д.В. Перевозкин, А.В. Петухов...

5. Il’in V.P. Parallelnye metody i tekhnologii decompozitsii oblastey [Parallel methods
and technologies of domain decomposition]. Vestnik YUURGU. Seriya “Vychislitelnaya
matematika i informatika” [Bulletin of South Ural State University. Seriers: Computational
Mathematics and Software Engineering], 2012. No. 46(305). P. 31–44.

6. Il’in V.P., Knysh D.V. Parallelnye metody decompozitsii v prostranstvakh sledov [Parallel
decomposition methods in trace spaces]. Vychislitelnye metody i programmirovanie
[Computational methods and programming], 2011. Vol. 12, No. 1. P. 100–109.

7. Saad Y. Iterative Methods for Sparse Linear Systems, Second Edition. SIAM, 2003.
8. Intel Math Kernel Library. Reference Manual. URL: http://software.intel.com/

sites/products/documentation/doclib/mkl_sa/11/mklman/index.htm (accessed:
12.02.2013).

9. Il’in V.P. Metody bisopryazhennykh napravleniy v podprostranstvakh Krylova [Bi-conjugate
directions methods in Krylov subspaces]. SibZHIM [Syberian Journal of Industrial
Mathematics], 2008. Vol. 11, No. 4(36). P. 47–60.

10. Bell, N. Cusp: Generic Parallel Algorithms for Sparse Matrix and Graph Computations /
N. Bell, M. Garland. URL: http://cusp-library.googlecode.com (accessed: 15.10.2012).

11. PETSc: Home Page. URL: http://www.mcs.anl.gov/petsc/ (accessed: 12.02.2013).
12. Hypre. URL: http://acts.nersc.gov/hypre/ (accessed: 12.02.2013).
13. Yousef Saad – Software. URL: http://www-users.cs.umn.edu/~saad/software/ (accessed:

12.02.2013).
14. Klaster HKC-30T [Cluster HKC-30T].

URL: http://www2.sscc.ru/HKC-30T/HKC-30T.htm (accessed: 12.02.2013).
15. Nabben R., Vuik C. A comparison of abstract versions of deflation, balancing and additive

coarse grid correction preconditioners // Numerical Linear Algebra with Applicati 2008. Vol.
15, No 4. P. 355–372.

Поступила в редакцию 14 июня 2013 г.

2013, т. 2, № 3 105