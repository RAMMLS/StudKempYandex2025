Моделирование нейронной сети для решения задачи классификации… 

УДК 533.6                                     DOI: 10.18698/2309-3684-2018-4-5771 
 
Моделирование нейронной сети для решения задачи 

классификации элементов корпуса 
летательного аппарата 

© В.Н. Булгаков1,2,  Р.А. Рацлав2, 
Д.А. Сапожников1,2,  И.В. Чернышев1,2 

1АО «ВПК «НПО машиностроения», 
г. Реутов, Московская обл., 143966, Россия 

2МГТУ им. Н.Э. Баумана, Москва, 105005, Россия 
 
Реализована модель нейронной сети для выполнения классификации элементов по-
верхностей летательного аппарата. Сгенерирована выборка, содержащая пара-
метры поверхностей объектов классификации. Для того чтобы избежать оши-
бок, связанных с разными шкалами измерения, признаки были масштабированы. 
По синтетическим данным проведено обучение нейронной сети. Верификация 
предлагаемой модели проведена также с помощью синтетических данных. Опти-
мальная конфигурация нейронной сети определена экспериментально. В качестве 
критерия оптимальности была использована доля правильных ответов тестовой 
и обучающей выборок. Были проведены калибровка и модификация отдельных па-
раметров модели. Результаты классификации тестовой выборки оптимальной 
сетью сведены в матрицу ошибок. Наиболее значимый результат достигнут при 
отделении класса эллипсоидов. Отдельные блоки матрицы показывают, что 
нейронная сеть безошибочно отделяет классы эллипсоидов и гиперболоидов. 
Предложены идеи для дальнейшей модификации алгоритма в целях повышения до-
ли правильных ответов при отделении класса параболоидов. 
 
Ключевые слова: летательный аппарат, нейронная сеть, метод обратного рас-
пространения ошибки, многослойный персептрон, классификация, оптимизация 
аэродинамических расчетов. 

 
Введение. В большинстве случаев летательный аппарат (ЛА) 

представляет собой комбинацию поверхностей первого и второго по-
рядков. Чаще других встречаются цилиндрические, конические, эл-
липтические (в частности, сферические) поверхности, плоские про-
фили, элементы двуполостных и эллиптических параболоидов [1]. На 
данный момент существуют методы определения аэродинамических 
характеристик, которые работают с телами, заданными в виде масси-
ва точек, связанных между собой в четырехугольные ячейки расчет-
ной поверхности сетки [2–4]. Для задания начального приближения 
на этой сетке используют метод Ньютона, определяющий давление 
на элементе поверхности как функцию угла встречи потока с поверх-
ностью тела [1]. В то же время на разных элементах с одинаковым 
углом встречи давление может быть разным. Информации, которую 
несет одна ячейка, недостаточно для определения типа принадлежа-
щей ей поверхности. Проблема отделения классов конуса, цилиндра, 

57 



В.Н. Булгаков, Р.А. Рацлав, Д.А. Сапожников, И.В. Чернышев 

сферы, плоского профиля была решена в работе [5] с применением 
древовидного классификатора. Тем не менее даже с его помощью 
нельзя однозначно разделить линейным образом классы эллипсоида, 
гиперболоида и параболоида, хотя для их отделения были разрабата-
ны дополнительные признаки. В данной работе описывается процесс 
моделирования искусственной нейронной сети (ИНС) прямого рас-
пространения [6–14] с использованием многослойного персептрона 
[15–20], обучаемого методом обратного распространения на основе 
прецедентов для определения класса [21] поверхности летательного 
аппарата с учетом информации о соседних ячейках. 

Цель работы — создать классификатор, способный с достаточной 
точностью распознать и классифицировать элементы ЛА по классам 
эллипсоида, параболоида и гиперболоида без разработки дополни-
тельных признаков на основе имеющейся информации. 

Подход к решению. Для решения задачи классификации элемен-
тов летательного аппарата была построена искусственная нейронная 
сеть прямого распространения с одним скрытым слоем на базе мно-
гослойного персептрона. В качестве функции активации была ис-

пользована f AKT = th(x).  Выбор в качестве функции активации ги-
перболического тангенса, а не логистического сигмоида обусловлен 
областью определения входных сигналов (−1, 1)  и предполагаемым 
ускорением сходимости обучения [6]. Ввиду того что объектом клас-
сификации является минимальный щит, состоящий из 9 точек,  
описываемый 27 координатами, входной слой будет представлен  
28 нейронами: 27 — для признакового описания объекта, а 1 — 
нейрон смещения (bias) [10]. Подробнее объект классификации будет 
описан ниже. Между нейронами входного и скрытого слоев суще-
ствуют синаптические связи. Для каждой связи определен свой вес. 
Веса записывают в матрицу размерностью [I × H ],  где I — число 
нейронов входного слоя, H — число нейронов скрытого слоя. Мат-
рица имеет вид  

w11 w12 … w1H 
 

ih w21 w22 w
=  2H

W .  
 ⋮ ⋱ ⋮ 
 
wI1 wI 2 … wIH 

Здесь wij  — весовой коэффициент при передаче сигнала от j-го 

нейрона входного слоя i-му нейрону скрытого слоя. 
Сигнал проходит нейроны входного слоя без изменений (функ-

ция активации входного нейрона линейная) [7]. Нейроны скрытого 
слоя принимают сигналы от нейронов входного слоя и суммируют их 
согласно соотношению 

58 



Моделирование нейронной сети для решения задачи классификации… 

I
H =∑ I

U w , IH
i ijU j + wbi w∈W ,                             (1) 

j=1

где H
U  — сигнал в i-м нейроне скрытого слоя; I

i U j  — сигнал, исхо-

дящий из j-го нейрона входного слоя; wbi  — вес связи между i-м 

нейроном и нейроном смещения на входном слое. 
Скрытый слой представлен Н нейронами, один из которых — 

нейрон смещения. Количество нейронов на скрытом слое мы будем 
варьировать для обучения и во избежание переобучения нейросети, 
точное их количество определим далее экспериментальным путем. 
Нейроны скрытого слоя также соединены синаптически с нейронами 
выходного слоя: 

 w11 w12 … w1O 
 

ho = 
w21 w22 w2O

W .  
 ⋮ ⋱ ⋮ 
 
wH1 wH 2 … wHO 

Сигнал в каждом скрытом нейроне проходит через функцию ак-
тивации, соответственно незначительное перемещение сигнала не 
окажет существенного влияния на значение сигнала в нейронах вы-
ходного слоя [8].  

Выходной слой представлен тремя нейронами по числу пред-
ставленных классов. Сигналы от всех скрытых нейронов суммируют-
ся в каждом выходном нейроне и проходят через функцию активации 
согласно соотношению 

H
O AKT ( H

U =∑w j f U ) + b , HO
i i j w i w∈W .                      (2) 

j=1

Результатом работы алгоритма будет вектор длины три 

d = {di} ,  i =1,3,  di <1.  

Решение принимается на основе стратегии «победитель получает 
все» [5]: объект относится к тому классу, чей выходной нейрон обла-
дает сигналом, наиболее близким к единице. Схема нейронной сети 
представлена на рис. 1.  

Объект классификации. Объекты классификации — четырех-
угольные ячейки сетки на телах разной формы. Очевидно, что по че-
тырем точкам невозможно однозначно определить класс, к которому 
относится наш объект. Известно, что с помощью инвариантов обоб-
щенного уравнения поверхности по набору из девяти точек мы мо-
жем однозначно определить класс поверхности. Поэтому в качестве 

59 



В.Н. Булгаков, Р.А. Рацлав, Д.А. Сапожников, И.В. Чернышев 

основной информации, с которой наша нейронная сеть будет рабо-
тать, мы будем использовать координаты девяти точек, которые взя-
ты с вершин четырех соседних площадок, образуя минимальный щит 
для классификации (рис. 2). В отличие от метода, предложенного  
в работе [5], при использовании ИНС не возникает необходимости  
в разработке новых признаков. 

 

Рис. 1. Схема нейронной сети 

 

Рис. 2. Объект классификации 

60 



Моделирование нейронной сети для решения задачи классификации… 

Обучающая выборка. Для обучения нейронной сети использо-
вали типичные для решения данной задачи учебные данные. Для их 
создания случайным образом генерировали коэффициенты канониче-
ских уравнений поверхностей. С этих поверхностей были взяты ко-
ординаты четырехугольных ячеек, котрорые затем зашумлялись, что 
означает искажение зашумляемого параметра не более чем на 1 %. 
Такие действия предназначены для приближения к реальным откло-
нениям в процессе взятия координат с математической модели лета-
тельного аппарата. Полученные зашумленные данные вместе с по-
рядковым номером класса поверхности построчно записывают в 
выборку. Для достаточно точного обучения нейронной сети сгенери-
рована выборка из 500 элементов. Выборка для тестирования сети 
состоит из 100 элементов.  

В качестве примера генерирования данных возьмем элемент  
эллиптического параболоида. Уравнение поверхности с единичными 
коэффициентами запишем в виде 

2 2

= y
x + z

.  
2 2

В сферической системе координат с зенитом, отсчитывающимся 
от оси Х, это уравнение будет примет вид 

x = r cosθ;

y = r sin θcos ϕ;  

z = r sin θsin ϕ;

1 2 2 2 1
r cos θ = r sin θcos ϕ + r2 sin2 θsin2 ϕ;

2 2
1

cosθ = sin2
r θ;  

2

= cosθ
r 2 .

sin2 θ
Для создания девяти случайных точек сгенерируем шаги по угло-

вым координатам — ∆θ, ∆ϕ  и r  как функцию от них  

r (ϕ,θ + 2∆θ) r (ϕ + ∆ϕ,θ + 2∆θ) r (ϕ + 2∆ϕ,θ + 2∆θ)

 r (ϕ,θ + ∆θ) r (ϕ + ∆ϕ,θ + ∆θ) (ϕ + 2∆ϕ,θ + ∆θ 

r )  .  

 r (ϕ,θ) r (ϕ + ∆ϕ,θ) r (ϕ + 2∆ϕ,θ) 
В декартовой системе координат мы получим: 

2

= cos θ
x 2 ;  

sin2 θ

61 



В.Н. Булгаков, Р.А. Рацлав, Д.А. Сапожников, И.В. Чернышев 

y = cos θcos ϕ
2 ;

sin θ
 

z = cosθsin ϕ
2 .

sin θ
Для каждой координаты случайно сгенерируем коэффициенты 

поверхности a, b  (в диапазоне [0; 10]  для обучающей выборки), 

сдвиговые константы x0 , y0 , z0  (диапазоны зависят от конкретного 

типа поверхности) и шумы Wx , Wy , Wz  в диапазоне [0,99;1,01] , что 

соответствует отклонению в 1 % (погрешность построения сеток, как 
правило, на порядки ниже этого значения [6]), и применим к каждой 
точке операторы сдвига, сжатия и зашумления: 

x′ Wx 0 0 0 1 0 0 x0  1 0 0 0 x
 ′  

0 0 0      
 y  0 1 0 0 0 0 0

= 
Wy   y   a   y .  

 z′  0 0 W 0 0 0 1 z0  0 0 b 0  z 
z

         
 1   0 0 0 1 0 0 0 1  0 0 0 1 1

Коэффициенты и сдвиги генерируют для всего набора точек, шу-
мы — отдельно для каждой точки. 

После всех проделанных операций сжатия и сдвигов мы получа-
ем набор точек, каждая из которых удовлетворяет уравнению по-
верхности, а шумы создают отклонение для имитации сетки взятой  
с реального тела.  

Масштабирование данных. Поскольку объекты описываются не-
зависимыми признаками, которые теоретически могут изменяться по 
различным шкалам, возникает необходимость приведения данных к 
одной шкале, так, например, элементы сферических затуплений нахо-
дятся в носовой области летательного аппарата, и их удаленность от 
начала координат существенно меньше, чем у плоских профилей, рас-
полагающихся в срединной области и в хвосте. Нейронная сеть при 
обучении на таких данных может счесть удаленность от начал коор-
динат атрибутом, важным для классификации, и принять решение ис-
ходя из этого обстоятельства, что в общем случае неверно [9]. 

Данные для обучения ИНС приводили к одной шкале таким обра-
зом, что любой k-й параметр имел математическое ожидание, равное 
нулю, и дисперсию, равную единице. 

xi − xi
i

xk = k cp
,  

xi
max

62 



Моделирование нейронной сети для решения задачи классификации… 

где i
xk  — приведенное значение i-го параметра k-го объекта; i

xcp  — 

среднее значение i-го параметра по всем объектам выборки; i
xmax  — 

максимальное (по модулю) значение i-го параметра по всем объектам 
выборки [7]. 

Сравнение обучения тестовой выборки с масштабированием  
и без него показано на рис. 3. На оси абсцисс отмечено число нейро-
нов скрытого слоя. 

 

Рис 3. Зависимость доли правильных ответов ИНС: 
 — с масштабированием;  — без масштабирования 

 
Прямое распространение. В качестве основополагающей кон-

цепции для моделирования нейронной сети используем персептрон. 
Эта модель восприятия информации состоит из датчиков, ассоциа-
тивных элементов и реагирующих элементов. Кроме того, она вклю-
чает в себя матрицу взаимодействия W, которая определяется после-
довательностью прошлых элементов сети. Сигнал движется от 
входных (сенсорных) элементов к выходным (реагирующим) элемен-
там через скрытые элементы, т. е. используется модель сети с пря-
мым распространением сигнала. Сигнал распространяется между 
слоями согласно формулам (1, 2). Ответы нейронной сети сверяются 
с целевыми значениями, определяется ошибка, для корректировки 
которой и необходимо обучение. Обучение осуществляется с помо-
щью метода обратного распространения ошибки, что позволяет отне-
сти нашу искусственную нейронную сеть к классу многослойного 
персептрона. 

Концепция обучения. Для обучения нейронной сети используют 
синтетическую выборку, описанную выше. Каждый обучающий 
пример согласуется с правильным ответом (обучение «с учителем»). 

63 



В.Н. Булгаков, Р.А. Рацлав, Д.А. Сапожников, И.В. Чернышев 

Цель обучения — изменение весовых коэффициентов для максими-
зации доли правильных ответов [10]. 

Обучение происходит в процессе 500 эпох (проходов по обучаю-
щей выборке). 

Завершающим этапом обучения искусственной нейронной сети 
будет тестирование и выявление признаков неправильной работы се-
ти. В ходе тестирования можно оценить ее способность обобщать по-
лученные знания. Под обобщением подразумевается способность се-
ти выполнять задачу распознавания класса поверхностей, данные 
которых аналогичны предоставленным сети данным в процессе обу-
чения, но все же отличаются от них. 

Нейроны скрытого слоя. Согласно теореме Колмогорова — 
Арнольда [15], любая функция нескольких переменных может быть 
представлена в виде нейронной сети с полными прямыми связями  
с одним скрытым слоем. Теорема, однако, не устанавливает точной 
зависимости числа нейронов скрытого слоя от числа нейронов вход-
ного и выходного слоев. Для определения оптимальной конфигура-
ции нейронной сети будем варьировать количество нейронов скрыто-
го слоя в каждом эксперименте. Оптимальной будем считать ту 
конфигурацию, при которой разность долей правильных ответов для 
обучающей и тестовой выборок стремится к минимуму, а доля пра-
вильных для тестовой выборки — к максимуму. 

Алгоритм обучения. В момент инициализации нейронной сети 
ее весовые коэффициенты задавали в виде случайных значений  
в диапазоне (−1,1) . Изменение весовых коэффициентов происходит 
по алгоритму обратного распространения ошибки. Для каждого обу-
чающего объекта нейронная сеть выдает ответ в форме вектора дли-
ны, соответствующей количеству нейронов выходного слоя. Ответ 
сравнивается с целевым вектором С, у которого корректному классу 
соответствует значение 1, остальным — значение –1. Вычисляется 
ошибка для каждого выхода [6]. В соответствии со значением ошиб-
ки градиентарным методом корректируются значения весовых коэф-
фициентов. Ошибку вычисляют по формуле 

δO = ( − O )( AKT ( O
i Ci i )′Ui f U ) ,                              (3) 

где Ci  — значение i-го элемента целевого вектора; O
Ui  — i-й ответ 

нейронной сети; ( AKT ( O
f Ui ))′  — значение производной функции  

активации для O
Ui .  

Веса обновляются следующим образом:  
n+1 = n

wij ij + ρδO O
i ∈ HO

w i U , wij W ,  

64 



Моделирование нейронной сети для решения задачи классификации… 

где ρ  — скорость обучения; весовые коэффициенты определены 
между скрытым и выходным слоями. 

На скрытом слое нам неизвестен целевой вектор, соответственно, 
мы не можем получить ошибку скрытого слоя в явном виде. Исполь-
зуем ошибку выходного слоя для определения ошибки скрытого слоя:  

 O
H 

δ = ∑δO O T
( AK H

k i wki f (Uk ))′ .                           (4) 
 i=1 

Веса между входным и скрытым слоями обновляются аналогич-
ным образом: 

n+1
w = n

ij wij + ρδH H
Ui , i ∈ IH

i w j W .  

Анализ результатов. Как было сказано выше, объем обучающей 
выборки составил 500 экземпляров, объем тестовой выборки —  
100 экземпляров. Количество эпох для обучения составило 500.  

Из данных рис. 4 можно сделать вывод, что сеть успевает обу-
читься за число эпох, существенно меньше чем 500. После прохож-
дения 120–130 эпох сеть практически не обучается. Также существу-
ет локальный максимум, соответствующий значению в 80 эпох, после 
которого происходит некоторая корректировка сети в сторону ухуд-
шения значений. 

 

Рис. 4. Зависимость доли правильных ответов ИНС  
от количества эпох обучения 

 
Количество нейронов скрытого слоя определялось эксперимен-

тально. Зависимость доли правильных ответов от числа нейронов 
скрытого слоя для обучающей и тестовой выборок приведена на  
рис. 5. Максимум доли правильных ответов для тестовой выборки, 
как и минимум разности долей правильных ответов для двух выбо-

65 



В.Н. Булгаков, Р.А. Рацлав, Д.А. Сапожников, И.В. Чернышев 

рок, приходится на число нейронов скрытого слоя, равное 5 и 6.  
В дальнейшем будем использовать конфигурацию с 6 нейронами 
скрытого слоя. 

 

Рис 5. Зависимость доли правильных ответов ИНС  
от числа нейронов скрытого слоя: 

 — обучающая;  — тестовая 

 
Для тестовой выборки построим матрицу ответов Т, где в i-й 

строке на j-й позиции разместим количество элементов класса i, от-
несенных классификатором к классу j. Соответственно, на диагонали 
будет располагаться количество элементов, классифицированных 
правильно: 

37 1 0 
T =  

 0 26 8 .  
 0 6 21 

Из матрицы видно, что элементы класса 1 (эллипсоид) хорошо 
отделились от элементов остальных классов, а основная доля ошибок 
приходится на элементы классов 2 (эллиптический параболоид) и 3 
(эллиптический гиперболоид). Доли правильных ответов приведены 
в таблице. 

Доли правильных ответов для каждого класса 

Класс Доля правильных ответов 

1 0,974 

2 0,765 

3 0,780 

66 



Моделирование нейронной сети для решения задачи классификации… 

Следует отметить, что ни один из объектов класса эллипсоид не 
был отнесен к классу гиперболоид (и наоборот), в то время как в рабо-
те [1] не удалось найти подходящий критерий для отделения этих 

классов. Если рассмотреть блок T1,3,  являющийся матрицей ответов 
для подзадачи отделения этих двух классов в рамках предложенной 
нейронной сети, то можно заметить, что в данном случае сеть не до-
пустила ошибок: 

1,3 37 0 
T =  .  

 0 21

Это свойство сети можно использовать для модификации метода, 
изложенного в работе [5].  

Кроме того, средняя доля правильных ответов, составившая 0,84, 
незначительно меньше доли правильных ответов [5], равной 0,95, од-
нако нейронная сеть не требовала теоретического подбора признаков  
и модификации признакового пространства. Ожидается, что даль-
нейшая модификация нейронной сети (подбор весовых коэффициен-
тов функции активации, целевое динамическое изменение скорости 
обучения для весов между скрытым и выходным слоями) значитель-
но увеличит долю правильных ответов. 

Выводы. Построена ИНС, которая с высокой точностью обеспе-
чивает отделение классов элементов летательного аппарата, для ко-
торых ранее не было обеспечено линейное разделение.  

Определена структура нейронной сети и обучены ее весовые ко-
эффициенты.  

Достигнутая доля правильных ответов (84 %) при отделении трех 
классов позволяет использовать ИНС данной конфигурации в зада-
чах классификации элементов корпуса летательного аппарата для 
дальнейшего целевого определения параметров обтекания с высокой 
точностью.  

Доля правильных ответов при отделении классов эллипсоида от 
гиперболоида, составившая на тестовой выборке 100 %, позволяет 
применять ИНС для модификации существующих алгоритмов отде-
ления данных классов. 

ЛИТЕРАТУРА 

[1] Краснов Н.Ф. Основы аэродинамического расчета. Аэродинамика тел вра-
щения, несущих и управляющих поверхностей. Аэродинамика летатель-
ных аппаратов. Москва, Высшая школа, 1981, 496 с. 

[2] Shinkyu J., Kazuhisa C., Shigeru O. Data minig for aerodynamic design space. 
Journal of Aerospace Computing, Information and Communication, 2005,  
vol. 2, no. 11, pp. 452–496. 

[3] Paul G.T. Advanced Computational Fluid and Aerodynamics. Cambridge, Cam-
bridge University Press, 2016, 578 p. 

67 



В.Н. Булгаков, Р.А. Рацлав, Д.А. Сапожников, И.В. Чернышев 

[4] Wei Wei, Rong Mo, Qingming Fan. Knowledge extraction for aerodynamic simu-
lation data compressor rotor. Procedia Engineering, 2011, no. 15, pp. 1792–1796. 

[5] Котенев В.П., Рацлав Р.А., Сапожников Д.А., Чернышев И.В. Метод клас-
сификации элементов поверхности летательного аппарата для численно-
аналитического решения задач аэродинамики. Математическое моделиро-
вание и численные методы, 2017, № 3, с. 83–104. 

[6] Каллан Р. Основные концепции нейронных сетей. Москва, Издательский 
дом «Вильямс», 2001, 287 с. 

[7] Хайкин С. Нейронные сети: полный курс. Москва, Издательский дом «Ви-
льямс», 2006, 1104 с. 

[8] Джонс М.Т. Программирование искусственного интеллекта в приложе- 
ниях. Москва, ДМК Пресс, 2011, 312 с. 

[9] Рутковская Д., Пилиньский М., Рутковский Л. Нейронные сети, генетиче-
ские алгоритмы и нечеткие системы. Москва, Горячая линия — Телеком, 
2006, 452 с. 

[10] Гольдштейн Б.С., Ехриель И.М., Рерле Р.Д. Интеллектуальные сети. 
Москва, Радио и связь, 2000, 500 с. 

[11] Круглов В.В., Борисов В.В. Искусственные нейронные сети. Теория  
и практика. Москва, Горячая линия — Телеком, 2002, 382 с. 

[12] Галушкин А.И. Теория нейронных сетей. Кн. 1: учебное пособие для вузов. 
Москва, ИПРЖР, 2000, 416 с. 

[13] Gallant S.L. Neutral Network Learning and Expert Systems. Cambridge, Massa-
chusetts, MIT Press., 1993, 364 p. 

[14] Sanger T.D. Optimal unsupervised learning in a single-layer linear feedforward 
neural network. Neural Networks, 1989, no. 2, pp. 459–473. 

[15] Колмогоров А.Н. О представлении непрерывных функций нескольких пе-
ременных в виде суперпозиции непрерывных функций одного переменно-
го. Доклады АН СССР, 1958, т. 114, № 5, с. 953–956. 

[16] Напалков А.В., Прагина Л.Л. Мозг человека и искусственный интеллект. 
Москва, Издательство МГУ, 1985, 120 с. 

[17] Пероуз Р. Новый ум короля: о компьютерах, мышлении и законах физики. 
Москва, Едитория УРСС, 2003, 384 с. 

[18] Minsky M., Papert S. Perceptrons: An Introduction to Computational Geometry. 
Cambridge, Massachusetts, MIT Press., 1969, 258 p. 

[19] David L.P., Alan K.M. Artificial intelligence. Cambridge University Press, 
2017, 760 p. 

[20] Rosenblatt F. Principles of Neurodynamics. New York, Spartan Books. 1962, 
616 p. 

[21] Hastie T., Tibshirani R., Friedman J. The Elements of Statistical Learning. Ed. 2, 
New York, Springer, 2009, 745 p. 

 
Статья поступила в редакцию 27.06.2018  

 
Ссылку на эту статью просим оформлять следующим образом: 
Булгаков В.Н., Рацлав Р.А., Сапожников Д.А., Чернышев И.В. Моделирование 

нейронной сети для решения задачи классификации элементов корпуса летательного 
аппарата. Математическое моделирование и численные методы, 2018, № 4, с. 57–71. 

 
Булгаков Владислав Николаевич — аспирант кафедры «Вычислительная мате-
матика и математическая физика» МГТУ им. Н.Э. Баумана, инженер отдела аэро-
динамики в АО «ВПК «НПО машиностроения», автор работ в области численных, 

68 



Моделирование нейронной сети для решения задачи классификации… 

аналитических и статистических методов исследования течения газа при обтекании 
поверхности летательных аппаратов. e-mail: v.n.bulgakov@vpk.npomash.ru  

 
Рацлав Роман Алексеевич — студент кафедры «Вычислительная математика и 
математическая физика» МГТУ им. Н.Э. Баумана. 

 
Сапожников Денис Алексеевич — аспирант кафедры «Вычислительная матема-
тика и математическая физика» МГТУ им. Н.Э. Баумана, инженер 2-й категории 
отдела аэродинамики в АО «ВПК «НПО машиностроения», автор работ в области 
численных, аналитических и статистических методов исследования течения газа 
при обтекании поверхности летательных аппаратов.  
e-mail: d.a.sapozhnikov@vpk.npomash.ru 
 
Чернышев Игорь Владимирович — студент кафедры «Вычислительная матема-
тика и математическая физика» МГТУ им. Н.Э. Баумана, техник АО «ВПК «НПО 
машиностроения». 

 

Modeling a neural network to solve the problem  
of classifying air frame elements 

© V.N. Bulgakov1,2, R.A. Ratslav2, 
D.A. Sapozhnikov1,2, I.V. Chernyshev1,2 

1JSC “MIC “NPO Mashinostroyenia”, Moscow Region, 
Reutov, 143966, Russia 

2Bauman Moscow State Technical University, Moscow, 105005, Russia 
 
The paper introduces a neural network implemented to classify air frame surface ele-
ments. Within the research, we generated a sample containing the surface parameters of 
classification objects. In order to avoid errors associated with different measurement 
scales, the criteria were scaled. According to synthetic data, the neural network was 
trained, and the proposed model was verified. The optimal configuration of the neural 
network was determined experimentally. As a criterion of optimality, we used the propor-
tion of correct answers from the test and training samples, and carried out calibration 
and modification of individual model parameters. The classification results of the test 
sample by the optimal network were summarized in the error matrix. The most significant 
result was achieved when distinguishing the class of ellipsoids. Separate blocks of the 
matrix show that the neural network accurately distinguishes the classes of ellipsoids and 
hyperboloids. The study proposes some ideas for further modification of the algorithm in 
order to increase the proportion of correct answers when distinguishing the class of pa-
raboloids. 
 
Keywords: aircraft, neural network, backpropagation, multilayer perceptron, classifica-
tion, optimization of aerodynamic calculations 

REFERENCES 

[1] Krasnov N.F. Osnovy aerodinamicheskogo rascheta. Aerodinamika tel vrash-
cheniya, nesushchikh i upravlyayushchikh poverkhnostey. Aerodinamika le-
tatelnykh apparatov [Fundamentals of aerodynamic calculation. Aerodynamics 
of bodies of revolution, bearing and control surfaces. Aerodynamics of aircraft]. 
Moscow, Vysshaya shkola Publ., 1981, 496 p. 

69 



В.Н. Булгаков, Р.А. Рацлав, Д.А. Сапожников, И.В. Чернышев 

[2] Shinkyu J., Kazuhisa C., Shigeru O. Data minig for aerodynamic design space. 
Journal of Aerospace Computing, Information and Communication, 2005,  
vol. 2, no. 11, pp. 452–496. 

[3] Paul G.T. Advanced Computational Fluid and Aerodynamics. Cambridge, Cam-
bridge University Press, 2016, 578 p. 

[4] Wei Wei, Rong Mo, Qingming Fan. Knowledge extraction for aerodynamic simu-
lation data compressor rotor. Procedia Engineering, 2011, no. 15, pp. 1792–1796. 

[5] Kotenev V.P., Raclav R.A., Sapozhnikov D.A., Chernyshev I.V. Matemati- 
cheskoe modelirovanie i chislennye metody – Mathematical Modeling and Com-
putational Methods, 2017, no. 3, pp. 83–1044. 

[6] Callan R. The Essence of Neural Networks. Prentice Hall Europe, 1999, 232 p.  
[In Russ.: Callan R. Osnovnye kontseptsii nejronnyh setey. Moscow, Vilyams 
Publ., 2001, 287 p.]. 

[7] Haykin S. Neural Networks: A Comprehensive Foundation. Prentice Hall, 2nd ed., 
1998, 842 p.  [In Russ.: Haykin S. Neyronnye seti: polny kurs. Moscow, Vilyams 
Publ., 200, 1104 p.]. 

[8] Jones M.T. AI Application Programming. Programming Series, 2005, 496 p.   
[In Russ.: Jones M.T. Programmirovanie iskusstvennogo intellekta v prilozhe-
niiakh. Moscow, DMK-Press Publ., 2011, 312 p.]. 

[9] Rutkovskaya D., Pilinskiy M., Rutkovskiy L. Neyronnye seti, geneticheskie  
algoritmy i nechetkie sistemy [Neural networks, genetic algorithms and fuzzy 
systems]. Moscow, Goryachaya liniya – Telekom Publ., 2006, 452 p. 

[10] Goldstein, B.S., Ehriel, I.M., Rerle, R.D. Intellektualnye seti [Intelligent Net-
works]. Moscow, Radio i svyaz Publ., 2000, 500 p. (In Russ.) 

[11] Kruglov V.V., Borisov V.V. Iskusstvennye neyronnye seti. Teoriya i praktika  
[Iskusstvennye nejronnye seti. Teoriya i praktika]. Moscow, Goryachaya liniya – 
Telekom Publ., 2002, 382 p. 

[12] Galushkin A.I. Teoriya neyronnykh setey. Kn. 1: Uchebnoe posobie dlya vuzov 
[Theory of neural networks. Book 1: textbook for universities]. Moscow, 
IPRZHR Publ., 2000, 416 p. 

[13] Gallant S.L. Neutral Network Learning and Expert Systems. Cambridge, Massa-
chusetts, MIT Press., 1993, 364 p. 

[14] Sanger T.D. Optimal unsupervised learning in a single-layer linear feedforward 
neural network. Neural Networks, 1989, no. 2, pp. 459–473. 

[15] Kolmogorov A.N. Doklady AN SSSR (Proceedings of the USSR Academy of Scien- 
ces), 1958, vol. 114, no. 5, pp. 953–956. 

[16] Napalkov A. V., Pragina L. L. Mozg cheloveka i iskusstvenny intellekt [Human 
brain and artificial intelligence]. Moscow, MSU Publ., 1985, 120 p. 

[17] Penrose R. The Emperor's New Mind: Concerning Computers, Minds, and the 
Laws of Physics. Popular Science Ser., Oxford University Press, 1 ed., 2002,  
640 p.  [In Russ.: Penrose R. Novyy um korolya: o kompyuterakh, myshlenii  
i zakonakh fiziki. Moscow, Editoriya URSS Publ., 2003, 384 p.]. 

[18] Minsky M., Papert S. Perceptrons: An Introduction to Computational Geometry. 
Cambridge, Massachusetts, MIT Press., 1969, 258 p. 

[19] David L. P., Alan K. M. Artificial intelligence. Cambridge University Press, 
2017, 760 p. 

[20] Rosenblatt F. Principles of Neurodynamics. New York, Spartan Books. 1962, 
616 p. 

[21] Hastie T., Tibshirani R., Friedman J. The Elements of Statistical Learning. New 
York, Springer, 2nd ed., 2009, 745 p. 

70 



Моделирование нейронной сети для решения задачи классификации… 

Bulgakov V.N., post-graduate student, Department of Computational Mathematics and 
Mathematical Physics, Bauman Moscow State Technical University, Engineer, Department 
of Aerodynamics, JSC “MIC “NPO Mashinostroyenia”. Author of academic papers in the 
field of numerical, analytical and statistical methods of studying the gas streaming in the 
aircraft surface flow. e-mail: v.n.bulgakov@vpk.npomash.ru  
 
Ratslav R.A., student, Department of Computational Mathematics and Mathematical 
Physics, Bauman Moscow State Technical University. 
 
Sapozhnikov D.A., post-graduate student, Department of Computational Mathematics 
and Mathematical Physics, Bauman Moscow State Technical University, Engineer of the 
2nd category, Department of Aerodynamics, JSC “MIC “NPO Mashinostroyenia”. Author 
of academic papers in the field of numerical, analytical and statistical methods of study-
ing the gas streaming in the aircraft surface flow.  
e-mail: d.a.sapozhnikov@vpk.npomash.ru 
 
Chernyshev I.V., student, Department of Computational Mathematics and Mathematical 
Physics, Bauman Moscow State Technical University, technician, Department of Aerody-
namics, JSC “MIC “NPO Mashinostroyenia”. 
 
 
 
 

71